
---
layout: post
title: "üí¨ Local LLM RAG Chatbot"
date: 2025-11-11
categories: [AI, LLM, FastAPI, Streamlit, Ollama, ChromaDB]
tags: [RAG, LLM, AI, Python, Chatbot, OpenAI, Local AI]
---

A **Retrieval-Augmented Generation (RAG)** chatbot powered by **Ollama**, **ChromaDB**, and **FastAPI** --- with an optional **Streamlit UI** for interactive chat.
The system can run entirely **offline** using local LLMs (like `llama3`) or connect to **OpenAI** when available.

---

## üöÄ Features

‚úÖ **Completely local or hybrid AI**
- Works with **OpenAI API** or **Ollama** models (local inference)
- Auto-detects your available backend

‚úÖ **Retrieval-Augmented Generation (RAG)**
- Uses **ChromaDB** to store and retrieve document embeddings
- Answers based on your uploaded `.txt` data

‚úÖ **Unified backend**
- Built with **FastAPI** for fast RESTful communication
- Easy integration with other frontends

‚úÖ **Interactive UI**
- Beautiful **Streamlit chat interface**
- Maintains conversational history
- One-click Windows launch script

---

## üß† Architecture Overview

```text
+-----------------------+
|  Text Documents (.txt)|
+----------+------------+
           |
           v
     [ ingest.py ]
           |
           v
  +-----------------+
  |  ChromaDB Store |
  +-----------------+
           |
           v
     [ retriever.py ]
           |
           v
     [ app.py (FastAPI) ]
           |
           v
     [ ui_streamlit.py ]

```

* * * * *

üß© Components
-------------

| File | Description |
| --- | --- |
| **`ingest.py`** | Reads text files ‚Üí generates embeddings (OpenAI or Ollama) ‚Üí stores in ChromaDB |
| **`retriever.py`** | Retrieves top relevant documents for a given question |
| **`app.py`** | FastAPI backend for Q&A requests |
| **`ui_streamlit.py`** | Browser-based chat interface |
| **`start_chat.bat` / `start_chat.ps1`** | One-click launchers for Windows |
| **`requirements.txt`** | Full list of dependencies |

* * * * *

‚öôÔ∏è Installation
---------------

### 1Ô∏è‚É£ Clone the repository

```
git clone https://github.com/oleglihvoinen/llm-rag-example.git
cd llm-rag-example

```

### 2Ô∏è‚É£ Create a virtual environment

```
python -m venv .venv
.\.venv\Scripts\activate

```

### 3Ô∏è‚É£ Install dependencies

```
pip install -r requirements.txt

```

### 4Ô∏è‚É£ (Optional) Add your OpenAI API key

Create a `.env` file:

```
OPENAI_API_KEY=your_key_here

```

* * * * *

ü§ñ Ollama Setup
---------------

1.  [Download Ollama](https://ollama.com/download) for your platform

2.  Pull your preferred local model (e.g., Llama 3):

    ```
    ollama pull llama3

    ```

3.  Start the Ollama server:

    ```
    ollama serve

    ```

* * * * *

üß± Prepare Your Data
--------------------

Place your `.txt` documents inside the `data/` folder, then run:

```
python ingest.py

```

‚úÖ This creates a local ChromaDB database (`/chroma_db`) with your text embeddings.

* * * * *

üí¨ Run the Chatbot
------------------

### ü™Ñ Option 1 -- One-click (Windows)

Double-click:

```
start_chat.bat

```

Then open <http://localhost:8501> in your browser.

### üß© Option 2 -- Manual startup

```
ollama serve
uvicorn app:app --reload
streamlit run ui_streamlit.py

```

* * * * *

üß† Example Query
----------------

**Question:**

> What is this project about?

**Answer:**

> This project demonstrates a retrieval-augmented chatbot that uses local embeddings and large language models to answer questions based on custom documents.

* * * * *

üõ† Requirements
---------------

-   Python 3.10+

-   Ollama 0.12+

-   ChromaDB 0.5+

-   (Optional) OpenAI API Key

* * * * *

üì¶ Requirements File
--------------------

See `requirements.txt`:

```
fastapi
uvicorn
chromadb
openai
requests
python-dotenv
streamlit
pydantic
tiktoken

```

* * * * *

üåê Project Structure
--------------------

```
llm-rag-example/
‚îÇ
‚îú‚îÄ‚îÄ app.py
‚îú‚îÄ‚îÄ retriever.py
‚îú‚îÄ‚îÄ ingest.py
‚îú‚îÄ‚îÄ ui_streamlit.py
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ start_chat.bat
‚îú‚îÄ‚îÄ start_chat.ps1
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ example.txt
‚îî‚îÄ‚îÄ chroma_db/

```

* * * * *

üìò License
----------

This project is released under the **MIT License** --- free for personal and educational use.

* * * * *

üß© Credits
----------

Created by **Oleg Lihvoinen**\
Built with ‚ù§Ô∏è using:

-   [FastAPI](https://fastapi.tiangolo.com)

-   [Streamlit](https://streamlit.io)

-   [Ollama](https://ollama.com)

-   [ChromaDB](https://www.trychroma.com)

-   [OpenAI](https://platform.openai.com)

* * * * *

üñºÔ∏è Optional Screenshots
------------------------

| Screenshot | Description |
| --- | --- |
| ![Chat UI](assets/images/llm1.jpg) | Streamlit chatbot interface |
| ![Architecture](assets/images/llm2.png) | Project architecture diagram |

* * * * *

> üí° **Tip:** This setup allows you to run an entirely offline local AI assistant.\
> You can expand it to include PDFs, SQL data, or internal documents --- simply adapt `ingest.py` to your preferred data source.


---

